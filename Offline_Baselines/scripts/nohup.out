Traceback (most recent call last):
  File "/home/bd2/ANATS/Offline_Baselines/scripts/train.py", line 17, in <module>
    from lib.losses import loss_select
  File "/home/bd2/ANATS/Offline_Baselines/scripts/../lib/losses.py", line 3, in <module>
    import torch.nn as nn
ModuleNotFoundError: No module named 'torch.nn'
SD
Trainset:	x-(63058, 12, 705, 3)	y-(63058, 12, 705, 1)
Valset:  	x-(21019, 12, 705, 3)  	y-(21019, 12, 705, 1)
Testset:	x-(21020, 12, 705, 3)	y-(21020, 12, 705, 1)

Random seed = 233
--------- STAEformer ---------
{
    "num_nodes": 705,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0003,
    "milestones": [
        20,
        30
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 200,
    "model_args": {
        "num_nodes": 705,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
STAEformer                               [16, 12, 705, 1]          676,800
├─Linear: 1-1                            [16, 12, 705, 24]         96
├─Embedding: 1-2                         [16, 12, 705, 24]         6,912
├─Embedding: 1-3                         [16, 12, 705, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 705, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 705, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 705, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 705, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 705, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 705, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 705, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 705, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 705, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 705, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 705, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 705, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 705, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 705, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 705, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 705, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 705, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 705, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 705, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 705, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 705, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 705, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 705, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 705, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 705, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 705, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 705, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 705, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 705, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 705, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 705, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 705, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 705, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 705, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 705, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 705, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 705, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 705, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 705, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 705, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 705, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 705, 152]        304
├─Linear: 1-6                            [16, 705, 12]             21,900
==========================================================================================
Total params: 1,737,060
Trainable params: 1,737,060
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 16.96
==========================================================================================
Input size (MB): 1.62
Forward/backward pass size (MB): 8655.46
Params size (MB): 4.24
Estimated Total Size (MB): 8661.33
==========================================================================================

Loss: HuberLoss

2025-05-26 02:24:41.036840 Epoch 1  	Train Loss = 23.74057 Val Loss = 23.35901
2025-05-26 03:01:16.655396 Epoch 2  	Train Loss = 18.79018 Val Loss = 19.69727
2025-05-26 03:37:47.100370 Epoch 3  	Train Loss = 17.88443 Val Loss = 19.29543
2025-05-26 04:14:22.475442 Epoch 4  	Train Loss = 17.39267 Val Loss = 19.38161
2025-05-26 04:51:01.697829 Epoch 5  	Train Loss = 17.05535 Val Loss = 17.58271
2025-05-26 05:27:56.913864 Epoch 6  	Train Loss = 16.82227 Val Loss = 17.45199
2025-05-26 06:04:50.219220 Epoch 7  	Train Loss = 16.64985 Val Loss = 17.48733
2025-05-26 06:41:34.089731 Epoch 8  	Train Loss = 16.49254 Val Loss = 17.29472
2025-05-26 07:18:23.551978 Epoch 9  	Train Loss = 16.38292 Val Loss = 17.21513
2025-05-26 07:55:12.680225 Epoch 10  	Train Loss = 16.27231 Val Loss = 17.42850
2025-05-26 08:31:50.422212 Epoch 11  	Train Loss = 16.16794 Val Loss = 17.37116
2025-05-26 09:08:25.144262 Epoch 12  	Train Loss = 16.11974 Val Loss = 17.32495
2025-05-26 09:45:03.846703 Epoch 13  	Train Loss = 16.03372 Val Loss = 17.20215
2025-05-26 10:21:43.814139 Epoch 14  	Train Loss = 15.97283 Val Loss = 17.22720
2025-05-26 10:58:25.272174 Epoch 15  	Train Loss = 15.92037 Val Loss = 17.05843
2025-05-26 11:34:55.931937 Epoch 16  	Train Loss = 15.87339 Val Loss = 17.03919
2025-05-26 12:11:45.321918 Epoch 17  	Train Loss = 15.84264 Val Loss = 17.01995
2025-05-26 12:48:26.815466 Epoch 18  	Train Loss = 15.78178 Val Loss = 17.47023
2025-05-26 13:25:06.500416 Epoch 19  	Train Loss = 15.74413 Val Loss = 17.11528
2025-05-26 14:01:49.159014 Epoch 20  	Train Loss = 15.70199 Val Loss = 16.88837
2025-05-26 14:38:37.806397 Epoch 21  	Train Loss = 15.15385 Val Loss = 16.58849
2025-05-26 15:15:20.331865 Epoch 22  	Train Loss = 15.07499 Val Loss = 16.58034
2025-05-26 15:51:49.670698 Epoch 23  	Train Loss = 15.03807 Val Loss = 16.61624
2025-05-26 16:28:17.379814 Epoch 24  	Train Loss = 15.01256 Val Loss = 16.54027
2025-05-26 17:04:44.904410 Epoch 25  	Train Loss = 14.98894 Val Loss = 16.58142
2025-05-26 17:41:10.750761 Epoch 26  	Train Loss = 14.96918 Val Loss = 16.54187
2025-05-26 18:17:37.681375 Epoch 27  	Train Loss = 14.94731 Val Loss = 16.57852
2025-05-26 18:54:06.042749 Epoch 28  	Train Loss = 14.93083 Val Loss = 16.61333
2025-05-26 19:30:34.543528 Epoch 29  	Train Loss = 14.91176 Val Loss = 16.63422
2025-05-26 20:07:01.593182 Epoch 30  	Train Loss = 14.89667 Val Loss = 16.64932
2025-05-26 20:43:28.720990 Epoch 31  	Train Loss = 14.82141 Val Loss = 16.60833
2025-05-26 21:19:56.269905 Epoch 32  	Train Loss = 14.81031 Val Loss = 16.57819
2025-05-26 21:56:21.333894 Epoch 33  	Train Loss = 14.80492 Val Loss = 16.61552
2025-05-26 22:32:47.838199 Epoch 34  	Train Loss = 14.80314 Val Loss = 16.57504
Early stopping at epoch: 34
Best at epoch 24:
Train Loss = 15.01256
Train MAE = 15.60231, RMSE = 25.10042, MAPE = 9.53574
Val Loss = 16.54027
Val MAE = 17.01368, RMSE = 26.24978, MAPE = 10.31228
Model checkpoint saved to: ../saved_models/STAEformer/STAEformer-SD-2025-05-26-01-47-41.pt
--------- Test ---------
All Steps (1-12) MAE = 17.73431, RMSE = 27.43835, MAPE = 11.19620
Step 1 MAE = 14.23458, RMSE = 21.40734, MAPE = 9.16073
Step 2 MAE = 15.76090, RMSE = 23.83611, MAPE = 9.99105
Step 3 MAE = 16.48595, RMSE = 25.12195, MAPE = 10.38271
Step 4 MAE = 16.99236, RMSE = 26.05393, MAPE = 10.66142
Step 5 MAE = 17.42443, RMSE = 26.82564, MAPE = 10.93175
Step 6 MAE = 17.81094, RMSE = 27.50045, MAPE = 11.16853
Step 7 MAE = 18.16213, RMSE = 28.11466, MAPE = 11.36443
Step 8 MAE = 18.53115, RMSE = 28.73105, MAPE = 11.62462
Step 9 MAE = 18.87195, RMSE = 29.28172, MAPE = 11.87124
Step 10 MAE = 19.18926, RMSE = 29.78063, MAPE = 12.14113
Step 11 MAE = 19.51016, RMSE = 30.26887, MAPE = 12.39331
Step 12 MAE = 19.84005, RMSE = 30.75407, MAPE = 12.66467
Inference time: 218.70 s
